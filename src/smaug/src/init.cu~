#include "../include/cudapars.h"

/////////////////////////////////////
// standard imports
/////////////////////////////////////
#include <stdio.h>
#include <math.h>

#include "../../athena.h"
#include "../../defs.h"
/////////////////////////////////////
// kernel function (CUDA device)
/////////////////////////////////////
#include "../include/gradops_i.cuh"
#include "../include/init_user_i.cuh"


//*d_p,*d_w, *d_wnew, *d_wmod, *d_dwn1,  *d_wd

__global__ void init_parallel(struct params *p, Real *wnew, Real *wmod, 
    Real *dwn1, Real *wd, Real *wtemp, Real *wtemp1, Real *wtemp2)
{
  // compute the global index in the vector from
  // the number of the current block, blockIdx,
  // the number of threads per block, blockDim,
  // and the number of the current thread within the block, threadIdx
  // int i = blockIdx.x * blockDim.x + threadIdx.x;
  // int j = blockIdx.y * blockDim.y + threadIdx.y;

 int iindex = blockIdx.x * blockDim.x + threadIdx.x;
 // int index,k;
int ni=p->n[0];
  int nj=p->n[1];
#ifdef USE_SAC_3D
  int nk=p->n[2];
#endif


// Block index
    int bx = blockIdx.x;
   // int by = blockIdx.y;
    // Thread index
    int tx = threadIdx.x;
   // int ty = threadIdx.y;
    
  Real *u,  *v,  *h;

   int ord;
//enum vars rho, mom1, mom2, mom3, energy, b1, b2, b3;


  int i,j;
  int ip,jp;
  int ii[NDIM];
   int dimp=((p->n[0]))*((p->n[1]));

   
 #ifdef USE_SAC_3D
   int kp;
  dimp=((p->n[0]))*((p->n[1]))*((p->n[2]));
#endif  
/*   int ip,jp,ipg,jpg;

  #ifdef USE_SAC_3D
   kp=iindex/(nj*ni/((p->npgp[1])*(p->npgp[0])));
   jp=(iindex-(kp*(nj*ni/((p->npgp[1])*(p->npgp[0])))))/(ni/(p->npgp[0]));
   ip=iindex-(kp*nj*ni/((p->npgp[1])*(p->npgp[0])))-(jp*(ni/(p->npgp[0])));
#else
    jp=iindex/(ni/(p->npgp[0]));
   ip=iindex-(jp*(ni/(p->npgp[0])));
#endif */ 

  #ifdef USE_SAC_3D
   kp=iindex/(nj*ni);
   jp=(iindex-(kp*(nj*ni)))/ni;
   ip=iindex-(kp*nj*ni)-(jp*ni);
#else
    jp=iindex/ni;
   ip=iindex-(jp*ni);
#endif     

   

     ii[0]=ip;
     ii[1]=jp;
     #ifdef USE_SAC_3D
	   ii[2]=kp;
     #endif

     #ifdef USE_SAC_3D
       if(ii[0]<p->n[0] && ii[1]<p->n[1] && ii[2]<p->n[2])
     #else
       if(ii[0]<p->n[0] && ii[1]<p->n[1])
     #endif
	{
		//b[i+j*(p->n[0])]=0;

                 //Define b	

 


	//apply this special condition
	//initiate alfven wave propagtion 
	//if no initial config read

	    /*for(int f=0; f<NVAR; f++)
            { 		         
                          for(ord=0;ord<(2+3*(p->rkon==1));ord++)
                              wmod[fencode3_i(p,ii,f)+ord*NVAR*dimp]=0;
	    }*/


//	 __syncthreads();

			}

        	
	 __syncthreads();


    /* #ifdef USE_SAC_3D
       if(ii[0]<p->n[0] && ii[1]<p->n[1] && ii[2]<p->n[2])
     #else
       if(ii[0]<p->n[0] && ii[1]<p->n[1])
     #endif
     
               for(int f=vel1; f<NDERV; f++)
                    wd[fencode3_i(p,ii,f)]=0.0;
     

 __syncthreads(); */



     #ifdef USE_SAC_3D
      // if((p->readini==0) && ii[0]>1 && ii[1]>1  && ii[2]>1 && ii[0]<(p->n[0])-1 && ii[1]<(p->n[1])-1 && ii[2]<(p->n[2])-1)
         if((p->readini==0) && ii[0]<(p->n[0]) && ii[1]<(p->n[1])   && ii[2]<(p->n[2])) 
     #else
      // if((p->readini==0) && ii[0]>2 && ii[1]>2 && ii[0]<(p->n[0])-3 && ii[1]<(p->n[1])-3)  //this form for OZT test???? 
     
     
     //if((p->readini==0) && ii[0]>1 && ii[1]>1  && ii[0]<(p->n[0])-1 && ii[1]<(p->n[1])-1)  //this form for OZT test???? 
        if((p->readini==0) && ii[0]<(p->n[0]) && ii[1]<(p->n[1]))  //this form for BW test  //still issue here
     #endif
	{


            #ifdef ADIABHYDRO
		    if(i> (((p->n[0])/2)-2) && i<(((p->n[0])/2)+2) && j>(((p->n[1])/2)-2) && j<(((p->n[1])/2)+2) ) 
				;//w[fencode3_i(p,ii,rho)]=1.3;
            #else
                   // init_alftest (Real *w, struct params *p,int i, int j)
                   // init_alftest(w,p,i,j);
                   // init_ozttest (Real *w, struct params *p,int i, int j)
                   // init_ozttest(w,p,i,j);
                   // init_bwtest(w,p,i,j);

	           //default values for positions these may be updated by the initialisation routines
                   wd[fencode3_i(p,ii,delx1)]=(p->dx[0]);
		   wd[fencode3_i(p,ii,delx2)]=(p->dx[1]);
                   wd[fencode3_i(p,ii,pos1)]=(p->xmin[0])+ii[0]*(p->dx[0]);
		   wd[fencode3_i(p,ii,pos2)]=(p->xmin[1])+ii[1]*(p->dx[1]);
                 #ifdef USE_SAC_3D
		   wd[fencode3_i(p,ii,pos3)]=(p->xmin[2])+ii[2]*(p->dx[2]);
                   wd[fencode3_i(p,ii,delx3)]=(p->dx[2]);
                 #endif

                   //init_user_i(w,p,ii);  //initilise using w field

                   //commented out because spicule problem
                   //constructed on host
               if(p->mode!=3)
                   init_user_i(wmod,wd,p,ii);
           #endif

	

        }
	
	 __syncthreads();


       





     #ifdef USE_SAC_3D
       if(ii[0]<p->n[0] && ii[1]<p->n[1] && ii[2]<p->n[2])
     #else
       if(ii[0]<p->n[0] && ii[1]<p->n[1])
     #endif
	{
        /*for(int f=energyb; f<NVAR; f++)
             if(f != rhob)
                      w[fencode3_i(p,ii,f)]=0.0;*/
        //w[fencode3_i(p,ii,b2b)]=w[fencode3_i(p,ii,b3b)];
        for(int f=rho; f<NVAR; f++)
        {               
                  //wmod[fencode3_i(p,ii,f)]=w[fencode3_i(p,ii,f)];
                  //wmod[  (((3*(1+(p->rkon)))-1)*NVAR*dimp)+fencode3_i(p,ii,f)]=w[fencode3_i(p,ii,f)];              
                  dwn1[fencode3_i(p,ii,f)]=0;

                  //initial value of ord changed to 1 ensure have correct background fields set
                  for(ord=1;ord<(2+3*(p->rkon==1));ord++)
                  {
                              //only the wmod field is used w now redundant
                              wmod[fencode3_i(p,ii,f)+ord*NVAR*dimp]=wmod[fencode3_i(p,ii,f)];

                              //original version using w
                              //wmod[fencode3_i(p,ii,f)+ord*NVAR*dimp]=w[fencode3_i(p,ii,f)];
                              //wmod[fencode3_i(p,ii,b2b)+ord*NVAR*dimp]=w[fencode3_i(p,ii,b3b)];
                  }
  /*        int dir=0;
 for(int field=rho;field<=rho ; field++)
if( ii[0]<4 && (p->ipe)==0  && ((p)->it)==1 && ( isnan(wmod[fencode3_i(p,ii,field)])|| wmod[fencode3_i(p,ii,field)]==0 ))
        { 
    				printf("nant %d %d %d %d %lg %lg \n",ii[0],ii[1],field,dir, wmod[fencode3_i(p,ii,rho)],wmod[fencode3_i(p,ii,field)] );
}*/
                  
        }

        for(int f=tmp1; f<NTEMP; f++)
                 wtemp[fencode3_i(p,ii,f)]=0;


	/*for(int field=rho;field<=rho ; field++)
	if(  (p->ipe)==0  && (  wmod[fencode3_i(p,ii,field)]==0 ))
		{ 
	    				printf("nanti %d %d %d %d %lg %lg \n",ii[0],ii[1],field,0, wmod[fencode3_i(p,ii,rho)],wmod[fencode3_i(p,ii,field)+dimp*NVAR] );
	}*/


}

 __syncthreads();



}

__global__ void updatemod_parallel(struct params *p, Real *w, Real *wnew, Real *wmod, 
    Real *dwn1, Real *wd, Real *wtemp, Real *wtemp1, Real *wtemp2)
{
  // compute the global index in the vector from
  // the number of the current block, blockIdx,
  // the number of threads per block, blockDim,
  // and the number of the current thread within the block, threadIdx
  // int i = blockIdx.x * blockDim.x + threadIdx.x;
  // int j = blockIdx.y * blockDim.y + threadIdx.y;

 int iindex = blockIdx.x * blockDim.x + threadIdx.x;
 // int index,k;
int ni=p->n[0];
  int nj=p->n[1];
#ifdef USE_SAC_3D
  int nk=p->n[2];
#endif


// Block index
    int bx = blockIdx.x;
   // int by = blockIdx.y;
    // Thread index
    int tx = threadIdx.x;
   // int ty = threadIdx.y;
    
  Real *u,  *v,  *h;

   int ord;
//enum vars rho, mom1, mom2, mom3, energy, b1, b2, b3;


  int i,j;
  int ip,jp;
  int ii[NDIM];
   int dimp=((p->n[0]))*((p->n[1]));

   
 #ifdef USE_SAC_3D
   int kp;
  dimp=((p->n[0]))*((p->n[1]))*((p->n[2]));
#endif  
/*   int ip,jp,ipg,jpg;

  #ifdef USE_SAC_3D
   kp=iindex/(nj*ni/((p->npgp[1])*(p->npgp[0])));
   jp=(iindex-(kp*(nj*ni/((p->npgp[1])*(p->npgp[0])))))/(ni/(p->npgp[0]));
   ip=iindex-(kp*nj*ni/((p->npgp[1])*(p->npgp[0])))-(jp*(ni/(p->npgp[0])));
#else
    jp=iindex/(ni/(p->npgp[0]));
   ip=iindex-(jp*(ni/(p->npgp[0])));
#endif */ 

  #ifdef USE_SAC_3D
   kp=iindex/(nj*ni);
   jp=(iindex-(kp*(nj*ni)))/ni;
   ip=iindex-(kp*nj*ni)-(jp*ni);
#else
    jp=iindex/ni;
   ip=iindex-(jp*ni);
#endif     

   

     ii[0]=ip;
     ii[1]=jp;
     #ifdef USE_SAC_3D
	   ii[2]=kp;
     #endif






 
     #ifdef USE_SAC_3D
       if(ii[0]<p->n[0] && ii[1]<p->n[1] && ii[2]<p->n[2])
     #else
       if(ii[0]<p->n[0] && ii[1]<p->n[1])
     #endif
	{
        for(int f=rho; f<NVAR; f++)
        {               
                  for(ord=1;ord<(2+3*(p->rkon==1));ord++)
                  {
                              //wmod[fencode3_i(p,ii,f)+ord*NVAR*dimp]=w[fencode3_i(p,ii,f)];
                              wmod[fencode3_i(p,ii,f)+ord*NVAR*dimp]=wmod[fencode3_i(p,ii,f)];

                            // if(p->ipe==0    && f==rho)
                            //    printf("wmod,w %d %d %lg %lg\n",ii[0],ii[1],wmod[fencode3_i(p,ii,f)+ord*NVAR*dimp],w[fencode3_i(p,ii,f)]);
 
                  }
          int dir=0;


 //for(int field=rho;field<=rho ; field++)
//if( /*ii[0]<4 &&*/ (p->ipe)==0  && /*((p)->it)==1 &&*/ (/* isnan(wmod[fencode3_i(p,ii,field)])||*/ wmod[fencode3_i(p,ii,field)]==0 ))
//        { 
//    				printf("nant %d %d %d %d %lg %lg \n",ii[0],ii[1],field,dir, wmod[fencode3_i(p,ii,rho)],wmod[fencode3_i(p,ii,field)] );
//}
                  
        }




}

 __syncthreads();



}


 //initialise grid on the gpu
 //we currently don't do this to avoid use of additional memory on GPU
//set up a temporary grid

__global__ void gridsetup_parallel(struct params *p, Real *w, Real *wnew, Real *wmod, 
    Real *dwn1, Real *wd, Real *wtemp, Real *wtemp1, Real *wtemp2, int dir)
{
  // compute the global index in the vector from
  // the number of the current block, blockIdx,
  // the number of threads per block, blockDim,
  // and the number of the current thread within the block, threadIdx
  // int i = blockIdx.x * blockDim.x + threadIdx.x;
  // int j = blockIdx.y * blockDim.y + threadIdx.y;

 int iindex = blockIdx.x * blockDim.x + threadIdx.x;
 // int index,k;
int ni=p->n[0];
  int nj=p->n[1];
#ifdef USE_SAC_3D
  int nk=p->n[2];
#endif


// Block index
    int bx = blockIdx.x;
   // int by = blockIdx.y;
    // Thread index
    int tx = threadIdx.x;
   // int ty = threadIdx.y;
    
  Real *u,  *v,  *h;

   int ord;
//enum vars rho, mom1, mom2, mom3, energy, b1, b2, b3;


  int i,j;
  int ip,jp,kp;
  int ii[NDIM];
   int dimp=((p->n[0]))*((p->n[1]));
   kp=0;
   
 #ifdef USE_SAC_3D
 
  dimp=((p->n[0]))*((p->n[1]))*((p->n[2]));
#endif  
/*   int ip,jp,ipg,jpg;

  #ifdef USE_SAC_3D
   kp=iindex/(nj*ni/((p->npgp[1])*(p->npgp[0])));
   jp=(iindex-(kp*(nj*ni/((p->npgp[1])*(p->npgp[0])))))/(ni/(p->npgp[0]));
   ip=iindex-(kp*nj*ni/((p->npgp[1])*(p->npgp[0])))-(jp*(ni/(p->npgp[0])));
#else
    jp=iindex/(ni/(p->npgp[0]));
   ip=iindex-(jp*(ni/(p->npgp[0])));
#endif */ 

  #ifdef USE_SAC_3D
   kp=iindex/(nj*ni);
   jp=(iindex-(kp*(nj*ni)))/ni;
   ip=iindex-(kp*nj*ni)-(jp*ni);
#else
    jp=iindex/ni;
   ip=iindex-(jp*ni);
#endif     

   

     ii[0]=ip;
     ii[1]=jp;
     #ifdef USE_SAC_3D
	   ii[2]=kp;
     #endif


     #ifdef USE_SAC_3D
       if(ii[0]>0 && ii[0]<(p->n[0]-1) && ii[1]<p->n[1] && ii[2]<p->n[2])
     #else
       if(ii[0]<p->n[0] && ii[1]<p->n[1])
     #endif
     {


        switch(dir)
        {

    case 0:
           wtemp2[encode3p2_i(p,ip+1,jp+1,kp+1,tmpnui)]=wd[fencode3_i(p,ii,pos1)];
    break;
    case 1:
           wtemp2[encode3p2_i(p,ip+1,jp+1,kp+1,tmpnui1)]=wd[fencode3_i(p,ii,pos2)];
    break;
    #ifdef USE_SAC_3D
           case 2:
                        wtemp2[encode3p2_i(p,ip+1,jp+1,kp+1,tmpnui2)]=wd[fencode3_i(p,ii,pos3)];
           break;
     #endif
           }
     }


        	
	 __syncthreads();




       





}




 //initialise grid on the gpu
 //we currently don't do this to avoid use of additional memory on GPU
//calculate the dx values

__global__ void setupdx_parallel(struct params *p, Real *w, Real *wnew, Real *wmod, 
    Real *dwn1, Real *wd, Real *wtemp, Real *wtemp1, Real *wtemp2, int dir)
{
  // compute the global index in the vector from
  // the number of the current block, blockIdx,
  // the number of threads per block, blockDim,
  // and the number of the current thread within the block, threadIdx
  // int i = blockIdx.x * blockDim.x + threadIdx.x;
  // int j = blockIdx.y * blockDim.y + threadIdx.y;

 int iindex = blockIdx.x * blockDim.x + threadIdx.x;
 // int index,k;
int ni=p->n[0];
  int nj=p->n[1];
#ifdef USE_SAC_3D
  int nk=p->n[2];
#endif


// Block index
    int bx = blockIdx.x;
   // int by = blockIdx.y;
    // Thread index
    int tx = threadIdx.x;
   // int ty = threadIdx.y;
    
  Real *u,  *v,  *h;

   int ord;
//enum vars rho, mom1, mom2, mom3, energy, b1, b2, b3;


  int i,j;
  int ip,jp,kp;
  int ii[NDIM];
   int dimp=((p->n[0]))*((p->n[1]));

   
 #ifdef USE_SAC_3D
 
  dimp=((p->n[0]))*((p->n[1]))*((p->n[2]));
#endif  
/*   int ip,jp,ipg,jpg;

  #ifdef USE_SAC_3D
   kp=iindex/(nj*ni/((p->npgp[1])*(p->npgp[0])));
   jp=(iindex-(kp*(nj*ni/((p->npgp[1])*(p->npgp[0])))))/(ni/(p->npgp[0]));
   ip=iindex-(kp*nj*ni/((p->npgp[1])*(p->npgp[0])))-(jp*(ni/(p->npgp[0])));
#else
    jp=iindex/(ni/(p->npgp[0]));
   ip=iindex-(jp*(ni/(p->npgp[0])));
#endif */ 

  #ifdef USE_SAC_3D
   kp=iindex/(nj*ni);
   jp=(iindex-(kp*(nj*ni)))/ni;
   ip=iindex-(kp*nj*ni)-(jp*ni);
#else
    jp=iindex/ni;
   ip=iindex-(jp*ni);
#endif     

   

     ii[0]=ip;
     ii[1]=jp;
     #ifdef USE_SAC_3D
	   ii[2]=kp;
     #endif

   //calculate the dx values


	    switch(dir)
	    {
		     case 0:
		     #ifdef USE_SAC_3D
		       if( ii[0]>0 && ii[0]<(p->n[0])+1 && ii[1]>0 &&  ii[1]<(p->n[1])+1 && ii[2]>0 &&  ii[2]<(p->n[2])+1)
		     #else
		       if( ii[0]>0 && ii[0]<(p->n[0])+1  && ii[1]>0 && ii[1]<(p->n[1])+1)
		     #endif
	                wd[fencode3_i(p,ii,delx1)]=0.5*(wtemp2[encode3p2_i(p,ip+1,jp,kp,tmpnui)]-wtemp2[encode3p2_i(p,ip-1,jp,kp,tmpnui)]);
		     break;
	
		     case 1:
		     #ifdef USE_SAC_3D
		       if(ii[0]>0 && ii[0]<(p->n[0])+1 && ii[1]>0 &&  ii[1]<(p->n[1])+1 && ii[2]>0 &&  ii[2]<(p->n[2])+1)
		     #else
		       if(ii[0]>0 && ii[0]<(p->n[0])+1 && ii[1]>0 && ii[1]<(p->n[1])+1)
		     #endif
			wd[fencode3_i(p,ii,delx2)]=0.5*(wtemp2[encode3p2_i(p,ip,jp+1,kp,tmpnui)]-wtemp2[encode3p2_i(p,ip,jp-1,kp,tmpnui)]);
		     break;
		         
		     #ifdef USE_SAC_3D
		     case 2:

		       if(ii[0]>0 && ii[0]<(p->n[0])+1 && ii[1]>0 && ii[1]<(p->n[1])+1 && ii[2]>0 && ii[2]<(p->n[2])+1)
			wd[fencode3_i(p,ii,delx3)]=0.5*(wtemp2[encode3p2_i(p,ip,jp,kp+1,tmpnui)]-wtemp2[encode3p2_i(p,ip,jp,kp-1,tmpnui)]);
		     break;			
		     #endif
	     }
     
        	
	 __syncthreads();







       





}

 //initialise grid on the gpu
 //we currently don't do this to avoid use of additional memory on GPU
//intialise temporrary matrix needs t be completed
__global__ void zerotempv_parallel(struct params *p, Real *w, Real *wnew, Real *wmod, 
Real *dwn1,  Real *wd, Real *wtemp, Real *wtemp1, Real *wtemp2,  int dir)
{

  int iindex = blockIdx.x * blockDim.x + threadIdx.x;
  const int blockdim=blockDim.x;
  const int SZWT=blockdim;
  const int SZWM=blockdim*NVAR;
  int tid=threadIdx.x;
  Real maxt=0,max3=0, max1=0;
  int i,j,iv;
  int is,js;
  int index,k;
  int ni=p->n[0];
  int nj=p->n[1];
  Real dt=p->dt;
  Real dy=p->dx[1];
  Real dx=p->dx[0];


  
   int ip,jp;



  int ii[NDIM];
  int dimp=((p->n[0]))*((p->n[1]));
 #ifdef USE_SAC_3D
   int kp;
   Real dz=p->dx[2];
   dimp=((p->n[0]))*((p->n[1]))*((p->n[2]));
#endif  
   //int ip,jp,ipg,jpg;

  #ifdef USE_SAC_3D
   kp=iindex/(nj*ni);
   jp=(iindex-(kp*(nj*ni)))/ni;
   ip=iindex-(kp*nj*ni)-(jp*ni);
#else
    jp=iindex/ni;
   ip=iindex-(jp*ni);
#endif  

int bfac1,bfac2,bfac3;
//int bfac1=(field==rho || field>mom2)+(field>rho && field<energy);
//int bfac2= (field==rho || field>mom2);
//int bfac3=(field>rho && field<energy);
//int shift=order*NVAR*dimp;




//init temp1 and temp2 to zero 
//the compute element initialising n[0] or n[1] element must do +1 and +2
//this is because we fit the problem geometrically to nixnj elements 

     ii[0]=ip;
     ii[1]=jp;
     i=ii[0];
     j=ii[1];
     k=0;
     #ifdef USE_SAC_3D
	   ii[2]=kp;
           k=ii[2];
     #endif

     #ifdef USE_SAC_3D
       if(ii[0]<p->n[0] && ii[1]<p->n[1] && ii[2]<p->n[2])
     #else
       if(ii[0]<p->n[0] && ii[1]<p->n[1])
     #endif
    //set viscosities
   //if(i<((p->n[0])) && j<((p->n[1])))
   {


        for(int f=d1; f<=d3; f++)
     #ifdef USE_SAC_3D
                 wtemp2[encode3p2_i(p,ii[0],ii[1],ii[2],tmpnui)]=0;
     #else
                 wtemp2[encode3p2_i(p,ii[0],ii[1],k,tmpnui)]=0;
     #endif

      if(i==((p->n[0])-1))
      {
        wtemp2[encode3p2_i(p,i+1,j,k,tmpnui)]=0;
        wtemp2[encode3p2_i(p,i+2,j,k,tmpnui)]=0;
      }
      if(j==((p->n[1])-1))
      {
          wtemp2[encode3p2_i(p,i,j+1,k,tmpnui)]=0;
          wtemp2[encode3p2_i(p,i,j+2,k,tmpnui)]=0;
      }

     #ifdef USE_SAC_3D
      if(k==((p->n[2])-1))
      {
          wtemp2[encode3p2_i(p,i,j,k+1,tmpnui)]=0;
          wtemp2[encode3p2_i(p,i,j,k+2,tmpnui)]=0;
      }

     #endif
      if(j==((p->n[1])-1)  && i==((p->n[0])-1))
      {
          for(int di=0; di<2; di++)
             for(int dj=0; dj<2; dj++)
                   wtemp2[encode3p2_i(p,i+1+di,j+1+dj,k,tmpnui)]=0;
      }
     #ifdef USE_SAC_3D
      if(i==((p->n[0])-1)  && k==((p->n[2])-1))
      {
          for(int di=0; di<2; di++)
             for(int dk=0; dk<2; dk++)
                   wtemp2[encode3p2_i(p,i+1+di,j,k+1+dk,tmpnui)]=0;
      }
      #endif

    

     #ifdef USE_SAC_3D
      if(j==((p->n[1])-1)  && k==((p->n[2])-1))
      {
          for(int dk=0; dk<2; dk++)
             for(int dj=0; dj<2; dj++)
                   wtemp2[encode3p2_i(p,i,j+1+dj,k+1+dk,tmpnui)]=0;
      }
      #endif

     #ifdef USE_SAC_3D
      if(i==((p->n[0])-1) && j==((p->n[1])-1)  && k==((p->n[2])-1))
      {
          for(int dk=0; dk<2; dk++)
             for(int dj=0; dj<2; dj++)
               for(int di=0; di<2; di++)
                   wtemp2[encode3p2_i(p,i+1+di,j+1+dj,k+1+dk,tmpnui)]=0;
      }
      #endif

   }

}



/////////////////////////////////////
// error checking routine
/////////////////////////////////////
void checkErrors_i(char *label)
{
  // we need to synchronise first to catch errors due to
  // asynchroneous operations that would otherwise
  // potentially go unnoticed

  cudaError_t err;

  

  err = cudaThreadSynchronize();
  if (err != cudaSuccess)
  {
    char *e = (char*) cudaGetErrorString(err);
    fprintf(stderr, "CUDA Error: %s (at %s)", e, label);
  }
  
  err = cudaGetLastError();
  if (err != cudaSuccess)
  {
    char *e = (char*) cudaGetErrorString(err);
    fprintf(stderr, "CUDA Error: %s (at %s)", e, label);
  }

  


}

int cusync(struct params **p)
{

  #ifdef USE_GPUD
     
         for(int igid=0; igid<((*p)->npe); igid++)
         {
                (*p)->ipe=igid;
                cudaSetDevice((*p)->gpid[igid]) ;
                
  #endif
  cudaThreadSynchronize();
  #ifdef USE_GPUD
                 (*p)->ipe=0;
                 cudaSetDevice((*p)->gpid[0]) ;
          }
  #endif
  return 0;
}

int cusetgpu(struct params **p)
{
  #ifdef USE_GPUD
    if(((*p)->ipe)==-1)
    {
         for(int igid=0; igid<((*p)->npe); igid++)
                (*p)->gpid[igid]=igid ;
    }
    else
      cudaSetDevice((*p)->gpid[(*p)->ipe]) ;
                
  #endif
 
  return 0;
}


int cucopytogpu(DomainS *pD)
{

GridS *pG=(pD->Grid);
GPUParams *p=pG->gpuparams;
GPUData *gd=pG->gpudata;
ConsS ***U1=pG->U;

GPUparams **d_p;

Real **d_wnew
Real **d_wmod
Real **d_dwn1
Real **d_wd
Real **d_wtemp;
Real **d_wtemp1;
Real **d_wtemp2;

//copy params to gpudata
cudaMemcpy(*(pG->gpuparams), *p, sizeof(GPUParams), cudaMemcpyHostToDevice);

  cudaError_t code;
  int i, Nx3T, Nx2T, Nx1T;

  /* Calculate physical size of grid */
  #ifdef USE_SAC_3D 
  if (pG->Nx[2] > 1)
    Nx3T = pG->Nx[2] + 2*nghost;
  else
    Nx3T = 1;
  #endif

  if (pG->Nx[1] > 1)
    Nx2T = pG->Nx[1] + 2*nghost;
  else
    Nx2T = 1;

  if (pG->Nx[0] > 1)
    Nx1T = pG->Nx[0] + 2*nghost;
  else
    Nx1T = 1;

 buffer=(Real *)calloc(Nx1T*Nx2T,sizeof(Real));
  for(k=0; k<Nx3T; k++)
{
 for(j=0; j<Nx2T; j++)
for(i=0; i<Nx1T; i++)

#ifdef USE_SAC_3D
switch(field)
{
case 0:
     buffer[field][i+j*Nx1T]=U1.d[i][j][k].d;
     break;
case 1:
     buffer[field][i+j*Nx1T]=U1.M1[i][j][k];
     break;
case 2:
     buffer[field][i+j*Nx1T]=U1.M2[i][j][k];
     break;
case 3:
     buffer[field][i+j*Nx1T]=U1.M3[i][j][k];
     break;
case 4:
     buffer[field][i+j*Nx1T]=U1.E[i][j][k];
     break;
case 5:
     buffer[field][i+j*Nx1T]=U1.B1c[i][j][k];
     break;
case 6:
     buffer[field][i+j*Nx1T]=U1.B2c[i][j][k];
     break;
case 7:
     buffer[field][i+j*Nx1T]=U1.B3c[i][j][k];
     break;
}
#endif

#ifdef USE_SAC
switch(field)
{
case 0:
     buffer[field][i+j*Nx1T]=U1.d[i][j][k];
     break;
case 1:
     buffer[field][i+j*Nx1T]=U1.Mx[i][j][k];
     break;
case 2:
     buffer[field][i+j*Nx1T]=U1.My[i][j][k];
     break;
case 3:
     buffer[field][i+j*Nx1T]=U1.Mz[i][j][k];
     break;
}
#endif

    code = cudaMemcpy((pG->gpudata)+field*Nx1T*Nx2T*Nx3T+k*Nx1T*Nx2T, buffer, Nx1T*NX2T, cudaMemcpyHostToDevice);
    if(code != cudaSuccess) {
      ath_error("[copy_to_gpu_mem U] error: %s\n", cudaGetErrorString(code));
}


  /* Start copying rows of gas variables from host to device memory */
  for(i=0; i<Nx2T; i++) {
    code = cudaMemcpy(pG_gpu->U+i*Nx1T+nghost, pG->U[i], sizeof(Gas)*(Nx1T-nghost), cudaMemcpyHostToDevice);
    if(code != cudaSuccess) {
      ath_error("[copy_to_gpu_mem U] error: %s\n", cudaGetErrorString(code));
    }
    code = cudaMemcpy(pG_gpu->B1i+i*Nx1T+nghost, pG->B1i[i], sizeof(Real)*(Nx1T-nghost), cudaMemcpyHostToDevice);
    if(code != cudaSuccess) {
      ath_error("[copy_to_gpu_mem B1i] error: %s\n", cudaGetErrorString(code));
    }
    code = cudaMemcpy(pG_gpu->B2i+i*Nx1T+nghost, pG->B2i[i], sizeof(Real)*(Nx1T-nghost), cudaMemcpyHostToDevice);
    if(code != cudaSuccess) {
      ath_error("[copy_to_gpu_mem B2i] error: %s\n", cudaGetErrorString(code));
    }
    code = cudaMemcpy(pG_gpu->B3i+i*Nx1T+nghost, pG->B3i[i], sizeof(Real)*(Nx1T-nghost), cudaMemcpyHostToDevice);
    if(code != cudaSuccess) {
      ath_error("[copy_to_gpu_mem B3i] error: %s\n", cudaGetErrorString(code));
    }
  }


}

//int cuinit(struct params **p, struct bparams **bp, Real **wmod,Real **wnew, Real **wd, struct state **state, struct params **d_p, struct bparams **d_bp, Real **d_wnew, Real **d_wmod, Real **d_dwn1, Real **d_wd, struct state **d_state, Real **d_wtemp, Real **d_wtemp1, Real **d_wtemp2)

int cuinit(DomainS *pD)
{

GridS *pG=(pD->Grid);

/*Data and params pointer */
GPUParams **p;
GPUBParams **d_bp;
GPUData *gpudata;

cudaError_t code;
int i, Nx3T, Nx2T, Nx1T;

GPUparams **d_p;

Real *d_wnew
Real *d_wmod
Real *d_dwn1
Real *d_wd
Real *d_wtemp;
Real *d_wtemp1;
Real *d_wtemp2;

    d_wmod  =  gpudata->d_wmod;
    d_w  =  gpudata->d_w;
    d_wnew  =  gpudata->d_wnew;
    d_wn1  =  gpudata->d_wn1;
    d_wd  =  gpudata->d_wd;
    d_wtemp  =  gpudata->d_wtemp;
    d_wtemp1  =  gpudata->d_wtemp1;
    d_wtemp2  =  gpudata->d_wtemp2;

     d_bp =  gpudata->d_b;

/////////////////////////////////////
  // (1) initialisations:
  //     - perform basic sanity checks
  //     - set device
  /////////////////////////////////////

  printf("in cuinit\n");


  p=&(malloc(sizeof(GPUparams)));
  pG->gpuparams= *p;



  //fill the parameter block with the athena settings


  /* Calculate physical size of grid */
  #ifdef USE_SAC_3D 
  if (pG->Nx[2] > 1)
    Nx3T = pG->Nx[2] + 2*nghost;
  else
    Nx3T = 1;
  #endif


  if (pG->Nx[1] > 1)
    Nx2T = pG->Nx[1] + 2*nghost;
  else
    Nx2T = 1;

  if (pG->Nx[0] > 1)
    Nx1T = pG->Nx[0] + 2*nghost;
  else
    Nx1T = 1;

  (*p)->n[0]=Nx1T;
  (*p)->n[1]=Nx2T;
  #ifdef USE_SAC_3D
    (*p)->n[2]=Nx3T;
  #endif
 
  int dimp=(((*p)->n[0]))*(((*p)->n[1]));
   
  #ifdef USE_SAC_3D 
  	dimp=(((*p)->n[0]))*(((*p)->n[1]))*(((*p)->n[2]));
  #endif  

   (*p)->mode=0;
   (*p)->ng[0]=nghost;
   (*p)->ng[1]=nghost;
  #ifdef USE_SAC_3D
   (*p)->ng[2]=nghost; 
  #endif 






  (*p)->qt=pG->time;
  (*p)->dt=pG->dt;

  (*p)->xmax[0]=pG->MaxX[0]; 
  (*p)->xmin[0]=pG->MinX[0]; 
  (*p)->dx[0]=pG->dx1; 

 (*p)->xmax[1]=pG->MaxX[1]; 
  (*p)->xmin[1]=pG->MinX[1]; 
  (*p)->dx[1]=pG->dx2; 


  #ifdef USE_SAC_3D
   (*p)->xmax[2]=pG->MaxX[2]; 
  (*p)->xmin[2]=pG->MinX[2]; 
  (*p)->dx[2]=pG->dx3; 
 
  #endif 

/**/
(*p)->chyp3=par_getd("smaug","chyp3");


for(i=0;i<NVAR;i++)
  (*p)->chyp[i]=0.0;

(*p)->chyp[rho]=par_getd("smaug","chyprho");
(*p)->chyp[energy]=par_getd("smaug","chypenergy");
(*p)->chyp[b1]=par_getd("smaug","chypb1");
(*p)->chyp[b2]=par_getd("smaug","chypb2");
(*p)->chyp[mom1]=par_getd("smaug","chypmom1");
(*p)->chyp[mom2]=par_getd("smaug","chypmom2");

  #ifdef USE_SAC_3D
   (*p)->chyp[mom3]=par_getd("smaug","chypmom3");
   (*p)->chyp[b3]=par_getd("smaug","chypb3");
  #endif 

 // (p->boundtype[ii][idir][ibound])=0;  //period=0 mpi=1 mpiperiod=2  cont=3 contcd4=4 fixed=5 symm=6 asymm=7
for(int ii=0; ii<NVAR; ii++)
{
//period=0 mpi=1 mpiperiod=2  cont=3 contcd4=4 fixed=5 symm=6 asymm=7   
(*p)->boundtype[ii][0][0])=par_getd("smaug","boundu1");  
(*p)->boundtype[ii][1][0])=par_getd("smaug","boundu2");  
(*p)->boundtype[ii][0][1])=par_getd("smaug","boundl1");  
(*p)->boundtype[ii][1][1])=par_getd("smaug","boundl2");  
}


  //stores for fixed boundaries
  cudaMalloc((void **)&d_bp,sizeof(GPUBparams));
  #ifdef USE_SAC
	  cudaMalloc((void **) & ((*d_bp)->fixed1) , nghost*nx2T*NVAR*sizeof(Real));
	  cudaMalloc((void **) & ((*d_bp)->fixed2) , nghost*nx1T*NVAR*sizeof(Real));
  #endif

  #ifdef USE_SAC_3D
	  cudaMalloc((void **) & ((*d_bp)->fixed1) , nghost*nx2T*nx3T*NVAR*sizeof(Real));
	  cudaMalloc((void **) & ((*d_bp)->fixed2) , nghost*nx1T*nx3T*NVAR*sizeof(Real));
	  cudaMalloc((void **) & ((*d_bp)->fixed3) , nghost*nx1T*nx2T*NVAR*sizeof(Real));
  #endif

  //allocate the meory for the gpudata
if((*p)->mode != 3)
{
	if(((*p)->rkon)==1)
	  cudaMalloc((void**)d_wmod, 6*NVAR*dimp*sizeof(Real));
	else
	  cudaMalloc((void**)d_wmod, 3*NVAR*dimp*sizeof(Real));

	  cudaMalloc((void**)d_dwn1, NVAR*dimp*sizeof(Real));
	  cudaMalloc((void**)d_wd, NDERV*dimp*sizeof(Real));
	  cudaMalloc((void**)d_wtemp, NTEMP*dimp*sizeof(Real));


	  #ifdef USE_SAC
	  cudaMalloc((void**)d_wtemp1, NTEMP1*(((*p)->n[0])+1)* (((*p)->n[1])+1)*sizeof(Real));
	  cudaMalloc((void**)d_wtemp2, NTEMP2*(((*p)->n[0])+2)* (((*p)->n[1])+2)*sizeof(Real));
	  #endif
	  #ifdef USE_SAC_3D
	  cudaMalloc((void**)d_wtemp1, NTEMP1*(((*p)->n[0])+1)* (((*p)->n[1])+1)* (((*p)->n[2])+1)*sizeof(Real));
	  cudaMalloc((void**)d_wtemp2, NTEMP2*(((*p)->n[0])+2)* (((*p)->n[1])+2)* (((*p)->n[2])+2)*sizeof(Real));
	  #endif



	
	//printf("allocating %d %d %d %d\n",dimp,(*p)->n[0],(*p)->n[1],(*p)->n[2]);
	printf("allocating %d %d %d \n",dimp,(*p)->n[0],(*p)->n[1]);
        pG->gpudata=(GPUData *)malloc(sizeof(GPUData));
        gpudata=pG->gpudata;

        gpudata->d_w= *d_w;
        gpudata->d_wnew= *d_wnew;
        gpudata->d_wn1= *d_wn1;
        gpudata->d_wd= *d_wd;
        gpudata->d_wtemp= *d_wtemp;
        gpudata->d_wtemp1= *d_wtemp1;
        gpudata->d_wtemp2= *d_wtemp2;

        gpudata->d_b= *d_bp;


//copy params to gpudata
cudaMemcpy(*(pG->gpuparams), *p, sizeof(GPUParams), cudaMemcpyHostToDevice);

        cucopytogpu(pD);


	//printf("here1\n");






	 
	    //printf("here2\n");

	    //cudaMemcpy(*d_w, *w, NVAR*dimp*sizeof(Real), cudaMemcpyHostToDevice);
	    cudaMemcpy(*d_wmod, *wmod, 2*(1+(((*p)->rkon)==1))*NVAR*dimp*sizeof(Real), cudaMemcpyHostToDevice);
	    cudaMemcpy(*d_wd, *wd, NDERV*dimp*sizeof(Real), cudaMemcpyHostToDevice);






	//printf("here3\n");






	   // cudaMemcpy(*d_wnew, *wnew, 8*((*p)->n[0])* ((*p)->n[1])*sizeof(Real), cudaMemcpyHostToDevice);
	   // printf("here\n");
	    cudaMemcpy(*d_p, *p, sizeof(struct params), cudaMemcpyHostToDevice);
	    cudaMemcpy(*d_state, *state, sizeof(struct state), cudaMemcpyHostToDevice);
	    
	    dim3 dimBlock(16, 1);
	    //dim3 dimGrid(((*p)->n[0])/dimBlock.x,((*p)->n[1])/dimBlock.y);
	    dim3 dimGrid(((*p)->n[0])/dimBlock.x,((*p)->n[1])/dimBlock.y);
	   int numBlocks = (dimp+numThreadsPerBlock-1) / numThreadsPerBlock;
	   

	    printf("calling initialiser\n");
	     init_parallel<<<numBlocks, numThreadsPerBlock>>>(*d_p, *d_wnew, *d_wmod, *d_dwn1,  *d_wd, *d_wtemp, *d_wtemp1, *d_wtemp2);


}//end of if(p->mode !=3)


	    printf("called initialiser\n");
	//cudaMemcpy(*w, *d_w, NVAR*dimp*sizeof(Real), cudaMemcpyDeviceToHost);
if((*p)->mode !=3)
{
	cudaMemcpy(*state, *d_state, sizeof(struct state), cudaMemcpyDeviceToHost);
        cudaMemcpy(*p, *d_p, sizeof(struct params), cudaMemcpyDeviceToHost);
}




  return 0;



}




int cuupdatemod(struct params **p, struct bparams **bp,Real **w, Real **wnew, Real **wd, struct state **state, struct params **d_p, struct bparams **d_bp,Real **d_w, Real **d_wnew, Real **d_wmod, Real **d_dwn1, Real **d_wd, struct state **d_state, Real **d_wtemp, Real **d_wtemp1, Real **d_wtemp2)
{
  int deviceCount;
  int dir;
 
  printf("in cuinit\n");
 // Real *adb;
  Real *adw, *adwnew;
  struct params *adp;
  struct bparams *adbp;
  struct state *ads;

 
 
  int dimp=(((*p)->n[0]))*(((*p)->n[1]));

   
 #ifdef USE_SAC_3D   
  dimp=(((*p)->n[0]))*(((*p)->n[1]))*(((*p)->n[2]));
#endif  

   int numBlocks = (dimp+numThreadsPerBlock-1) / numThreadsPerBlock;
   
    printf("calling updatemod\n");
     //init_parallel(struct params *p, Real *b, Real *u, Real *v, Real *h)
    // init_parallel<<<dimGrid,dimBlock>>>(*d_p,*d_b,*d_u,*d_v,*d_h);
    // init_parallel<<<numBlocks, numThreadsPerBlock>>>(*d_p,*d_w, *d_wnew, *d_b);
     updatemod_parallel<<<numBlocks, numThreadsPerBlock>>>(*d_p,*d_w, *d_wnew, *d_wmod, *d_dwn1,  *d_wd, *d_wtemp, *d_wtemp1, *d_wtemp2);
     //cudaThreadSynchronize();
     


 


  return 0;
}





